---
title: Vikram Voleti's work experience
layout: default
permalink: /experience
---

| <a href="{{ site.google_scholar_url }}" target="_blank" style="text-align:center; display:block"><i class="ai ai-google-scholar-square ai-3x"></i></a> | <a href="https://linkedin.com/in/{{ site.linkedin_username }}" target="_blank" style="text-align:center; display:block"><i class="fa fa-linkedin ai-3x"></i></a> | <a href="https://github.com/{{ site.github_username }}" target="_blank" style="text-align:center; display:block"><i class="fa fa-github ai-3x"></i></a> |

<img class="experience-main" src="{{site.url}}{{site.baseurl}}/images/experience/experience.png">

# CURRENT

<!--

<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/guelph.png">](https://www.uoguelph.ca/engineering/){:target="_blank"}

_January 2020 --- present_

### [Prof. Graham Taylor](https://www.gwtaylor.ca/){:target="_blank"}, [University of Guelph](https://www.uoguelph.ca/engineering/){:target="_blank"} - Visiting Researcher
**Guelph, Canada**

I work in [Prof. Graham Taylor](https://www.gwtaylor.ca/){:target="_blank"}'s lab on computer vision and deep learning.

</div>
 -->

<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/stability_ai.png">](https://stability.ai/){:target="_blank"}

_April 2023 --- present_

### [Stability AI](https://stability.ai/){:target="_blank"} - Research Scientist
**Canada (Remote)**

I lead AI research and development on generating 3D objects, images, videos from text

</div>

<!-- <div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/mila_purple.png">](https://mila.quebec/en/){:target="_blank"}

_September 2018 --- September 2023_

### [Mila](https://mila.quebec/en/){:target="_blank"}, [University of Montreal](https://diro.umontreal.ca/){:target="_blank"} - PhD candidate

**PhD, Computer Science; Supervisor: [Prof. Christopher Pal](https://mila.quebec/en/person/pal-christopher/){:target="_blank"}**

My research interests are broadly in generative models and representation learning for images, video, 3D. I work on image, video and 3D generation using latent dynamics, and score-based diffusion models.
</div> -->


# EDUCATION

<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/mila_purple.png">](https://mila.quebec/en/){:target="_blank"}

_2018 --- 2023_

### [Mila](https://mila.quebec/en/){:target="_blank"}, [University of Montreal](https://diro.umontreal.ca/){:target="_blank"}, Canada

**PhD in Computer Science**
</div>


<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/kgp.jpg">](http://www.iitkgp.ac.in/){:target="_blank"}

_2009 --- 2014_

### [Indian Institute of Technology (IIT), Kharagpur](http://www.iitkgp.ac.in/){:target="_blank"}, India

**Dual Degree (B.Tech. (H) + M.Tech.) in Electrical Engineering**<br />with master's specialization in Instrumentation and Signal Processing
</div>


# EXPERIENCE

<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/stability_ai.png">](https://stability.ai/){:target="_blank"}

_April 2023 --- present_

### [Stability AI](https://stability.ai/){:target="_blank"} - Research Scientist
**Canada (Remote)**

I lead AI research and development on generating 3D objects, images, videos from text

</div>


<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/mila_purple.png">](https://mila.quebec/en/){:target="_blank"}

_2018 --- 2023_

### [Mila](https://mila.quebec/en/){:target="_blank"}, [University of Montreal](https://diro.umontreal.ca/){:target="_blank"} - Ph.D. student
**Montreal, Canada**

- Image generation using Multi-Resolution Continuous Normalizing Flows
- Image generation using Non-Isotropic Denoising Diffusion Models
- 3D animation using neural inverse kinematics with 3D human pose prior
- Video prediction using Neural ODEs
- Video prediction, generation, interpolation using Masked Conditional Video Diffusion models

</div>


<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/meta.png">](https://about.facebook.com/){:target="_blank"}

_August 2022 --- February 2023_

### [Meta](https://about.facebook.com/){:target="_blank"} - Research Intern
**Menlo Park, California, USA**

I worked with the [AI4AR](https://ai.facebook.com/blog/building-with-ai-across-all-of-meta/) team at [Meta](https://about.facebook.com/){:target="_blank"}, hosted by [Yashar Mehdad](https://scholar.google.com/citations?user=hFKgapkAAAAJ&hl=en).
• Led the technology development for generating 3D objects, videos from text; dreamfusion, NeRF
• Applied expertise at neural graphics for 3D rendering; implemented hands-on in PyTorch
• International AI team; technology transitioned into a Meta end product, adopted by other teams

</div>


<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/unity.png">](https://unity.com/labs){:target="_blank"}

_October 2021 --- August 2022_

### [Unity Technologies](https://unity.com/labs){:target="_blank"} - MITACS Research Intern
**Montreal, Canada**

I am a MITACS Research Intern with the DeepPose team at [Unity Labs](https://unity.com/labs){:target="_blank"}, hosted by [Boris Oreshkin](https://scholar.google.ca/citations?user=48MBCeIAAAAJ&hl=en){:target="_blank"}. I worked on 3D human pose estimation and inverse kinematics from videos.

| [SMPL-IK - Learned Morphology-Aware Inverse Kinematics for AI Driven Artistic Workflows](https://arxiv.org/abs/2208.08274){:target="_blank"}, *Vikram Voleti*, Boris N. Oreshkin, Florent Bocquelet, Félix G. Harvey, Louis-Simon Ménard, Christopher Pal - _[SIGGRAPH Asia 2022](https://sa2022.siggraph.org/en/){:target="_blank"}_ |

</div>


<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/diff3d.png">](http://montrealrobotics.ca/diff3d/){:target="_blank"}

_August 2021 --- October 2021_

### [Diff3D workshop](http://montrealrobotics.ca/diff3d/){:target="_blank"} @ [ICCV 2021](https://iccv2021.thecvf.com/home){:target="_blank"}- Organizer
**Canada**

I was a co-organizer of the Differentiable 3D Vision and Graphics workshop at ICCV 2021.
</div>


<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/owcv2021.png">](https://owcv2021.github.io){:target="_blank"}

_February 2021 --- April 2021_

### [OWCV 2021](https://owcv2021.github.io){:target="_blank"}, Canada - Organizer
**Canada**

I was a co-organizer of the inaugural Ontario Workshop on Computer Vision 2021.
</div>

<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/bluelion.jpeg">](https://bluelionlabs.com/){:target="_blank"}

_October 2020 --- March 2025_

### [Blue Lion Labs](https://bluelionlabs.com/){:target="_blank"} - AI Advisor
**Waterloo, Canada**

I advise [Blue Lion Labs](https://bluelionlabs.com/){:target="_blank"}, an early-stage startup that provides technology to automatically monitor different organisms in water using machine learning.

</div>

<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/graphquon.png">](https://graphquon.github.io){:target="_blank"}

_October 2020 --- December 2020_

### [GRAPHQUON 2020](https://graphquon.github.io){:target="_blank"}, Canada - Organizer
**Canada**

I was part of the organizing committee of GRAPHQUON 2020 (formerly MOTOGRAPH).
</div>


<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/UdeM.jpg">](https://www.umontreal.ca/){:target="_blank"}

_September 2020 --- December 2020_

### [Fundamentals of Machine Learning (IFT 6390)](http://mitliagkas.github.io/ift6390-ml-class/){:target="_blank"}, by [Ioannis Mitliagkas](http://mitliagkas.github.io/){:target="_blank"} - Teaching Assistant
**University of Montreal, Montreal, Canada**
</div>


<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/nextai.jpg">](https://www.nextcanada.com/next-ai){:target="_blank"}

_March 2020 --- September 2020_

### [NextAI](https://www.nextcanada.com/next-ai){:target="_blank"}, Toronto - AI Scientist in Residence
**Toronto, Canada**

I was a mentor/consultant for multiple startups at NextAI. I assisted them in integrating artificial intelligence and machine learning into their product pipeline, and with long-term strategies in technology.
</div>


<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/Google__G__Logo.svg">](https://ai.google/research/teams/perception/){:target="_blank"}

_September 2019 --- December 2019_

### [Google](https://ai.google/research/teams/perception/){:target="_blank"} - Research Intern
**Mountain View, California, USA**

I worked with the [Google AI Perception](https://ai.google/research/teams/perception/){:target="_blank"} team on deep models for large-scale video analysis for active speaker detection with [Bryan Seybold](https://ai.google/research/people/105552/){:target="_blank"} and [Sourish Chaudhuri](https://ai.google/research/people/SourishChaudhuri/){:target="_blank"}.

</div>


<div class="experience-box" markdown="1">

[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/IVADO.png">](https://ivado.ca/en/trainings/schools/deep-learning-school-4th-and-5th-edition/){:target="_blank"}

_September 9-13, 2019_

### [4th IVADO / Mila Deep Learning School](http://mitliagkas.github.io/ift6390-ml-class-2019/){:target="_blank"} - Teaching Assistant
**Montreal, Canada**

[Resources](https://github.com/mila-iqia/ivado-mila-dl-school-2019){:target="_blank"}
</div>


<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/UdeM.jpg">](https://www.umontreal.ca/){:target="_blank"}
_September 2019_

### [Fundamentals of Machine Learning (IFT 6390)](http://mitliagkas.github.io/ift6390-ml-class/){:target="_blank"}, by [Ioannis Mitliagkas](http://mitliagkas.github.io/){:target="_blank"} - Teaching Assistant
**University of Montreal, Montreal, Canada**
</div>


<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/nextai.png">](https://www.nextcanada.com/next-ai){:target="_blank"}

_April 2019 --- August 2019_

### [NextAI](https://www.nextcanada.com/next-ai){:target="_blank"}, Montreal - AI Scientist in Residence
**Montreal, Canada**

I was a mentor/consultant for 6 startups at NextAI. I assisted them in integrating artificial intelligence and machine learning into their product pipeline, and with long-term strategies in technology.
</div>



<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/playment.jpg">](https://playment.io){:target="_blank"}

_January 2018 --- June 2018_

### [Playment](https://playment.io){:target="_blank"} - Computer Vision Consultant
**Bengaluru, India**

Playment is a startup that offers annotation services for various computer vision tasks.

I was a consultant for the computer vision work at Playment. We focused on making more exhaustive and comprehensive semantic segmentation for autonomous driving using deep learning. We also worked at using classical computer vison as well as deep learning to solve various industrial problems including facial recognition, facial landmark detection, pedestrian detection.
</div>


<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/talent_sprint.png">](https://www.talentsprint.com/aiml.dpl){:target="_blank"}

_January 2018 --- May 2018_

### [IIIT-Hyderabad](https://cvit.iiit.ac.in){:target="_blank"} & [Talent Sprint](https://www.talentsprint.com/){:target="_blank"} - Mentor for Fundamentals of AI/ML
**Hyderabad, India**

I was a Mentor for the **Foundations of Artificial Intelligence and Machine Learning** certificate program by IIIT-H Machine Learning Lab and TalentSprint. I assisted in creating tutorials on machine learning, and mentor participants during lab sessions.
</div>


<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/iiith.png">](https://cvit.iiit.ac.in){:target="_blank"}

_May 2017 --- August 2018_

### [International Institute of Information Techonology (IIIT) - Hyderabad](https://cvit.iiit.ac.in){:target="_blank"} - Research Fellow
**Hyderabad, India** --- with [Prof. C. V. Jawahar](https://faculty.iiit.ac.in/~jawahar/){:target="_blank"}, Centre for Visual Information Technology, IIIT Hyderabad

- **Video Translation**
    - Generated videos of movies and educational tutorials of Andrew Ng in Indian languages by morphing lip movement
    - Experimented with GANs (Pix2Pix) to generate videos using original faces, new lip landmarks, and dubbed audio

- **Assessor for Lipreader**
    - Built a visual speech recognizer (lipreader) to classify spoken words in videos, and an assessor to check if the lipreader’s output is correct by combining convolutional and recurrent neural networks
    - Used the lipreader and assessor for self-training on unlabelled data, zero-shot learning on out-of-vocabulary words, and information retrieval

| [**Conference paper**]({{site.url}}{{site.baseurl}}/docs/publications/2018d_ICASSP_VisDub.pdf){:target="_blank"}: Abhishek Jha*, Vikram Voleti*, Vinay P. Namboodiri, C. V. Jawahar, "Cross-Language Speech Dependent Lip-Synchronization", in _[ICASSP 2019](https://2019.ieeeicassp.org/){:target="_blank"}_ [[IEEE](https://ieeexplore.ieee.org/document/8682275){:target="_blank"}] |

| [**Workshop paper**]({{site.url}}{{site.baseurl}}/docs/publications/2018b_CVPRW_VisDub.pdf){:target="_blank"}: Abhishek Jha*, Vikram Voleti*, Vinay P. Namboodiri, C. V. Jawahar, "Lip-Synchronization for Dubbed Instructional Videos", in _[CVPR Workshop (FIVER), 2018](http://fiver.eecs.umich.edu/#abstracts){:target="_blank"}_ |

</div>


<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/greyorange_logo.png">](http://www.greyorange.com/){:target="_blank"}

_Feb 2016 --- May 2017_

### [GreyOrange Robotics](http://www.greyorange.com/){:target="_blank"} - Image Processing Engineer

**Gurgaon, India**

GreyOrange Robotics is a multinational firm that designs, manufactures and deploys advanced robotics systems for automation at warehouses, distribution and fulfillment centres.

I was part of the Embedded Systems team. My job was to developed a computer vision module to perform video processing in real time for warehouse automation. We made an "Empty Carriage Detection System" (ECDS) for the "Cross-Belt Sorter" (CBS) that detects in real time whether a carriage in a conveyor belt has a packet on it or not, and relays the information to the server and mechanical systems. I also helped develop the embedded vision module in automated guided robots for warehouses, called "Butlers".

A [research paper]({{site.url}}{{site.baseurl}}/docs/publications/2017_ICIDE.pdf){:target="_blank"} based on some of the work has been accepted at the International Conference on Industrial Design Engineering, [ICIDE 2017](http://www.icide.org/){:target="_blank"}.

| [**Research paper**]({{site.url}}{{site.baseurl}}/docs/publications/2017_ICIDE.pdf){:target="_blank"}: V. Voleti, P. Mohan, S. Gupta, J. Iqbal, "Simple Real-Time Pattern Recognition for Industrial Automation", in _Proc. International Conference on Industrial Design Engineering_, 2017 |

</div>


<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/airbus.jpg">](http://www.airbus.com/){:target="_blank"}

_July 2014 --- Feb 2016_

### [Airbus, India](http://www.airbus.com/){:target="_blank"} - Associate Engineer

**Bengaluru, India**

Airbus is a commercial aircraft manufacturer, and the largest aeronautics & space company in Europe. I worked in the Bangalore (India) office as part of the Avionics Software and Systems Testing group. I was involved in development and integration of avionics systems in the Flight Warning Computer (FWC) for aircrafts in the long-range family.

I was part of the Avionics Software and Systems Testing group. My job was to simulate signal-level changes in the Flight Warning Computer, such as adding new signals for new functionalities, re-routing signals through different paths. This was followed by rigorous testing of the FWC for correct operation. We designed the re-routing paths, as well as the tests required to ensure all the functionalities of the FWC run correctly. For all development, standard avionics coding guidelines (DO-178B) were followed.
</div>


# THESIS PROJECTS

<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/kgp.jpg">](http://www.iitkgp.ac.in/){:target="_blank"}

_2013 --- 2014_

### Image De-fencing using Microsoft Kinect --- M.Tech. Thesis

[**IIT Kharagpur, India**](http://www.iitkgp.ac.in/){:target="_blank"} --- under [Prof. Rajiv Ranjan Sahay](http://www1.iitkgp.ac.in/fac-profiles/showprofile.php?empcode=STmUU&depts_name=EE){:target="_blank"}, Electrical Engineering

I worked on de-fencing of images using RGB-D data from Microsoft Kinect. We recorded images of scenes with fence-like occlusions and were successful in removing the fences from the scenes. We first recorded multpiple images of the same scene with slight spatial variation of the camera, and computed the approximate global shift among them. We then used loopy belief propagation to inpaint. A comparison of our technique and the erstwhile standards was made, and our method was demonstrated to be better.

A [research paper]({{site.url}}{{site.baseurl}}/docs/publications/2015_ICAPR.pdf){:target="_blank"} based on this work has been published in [IEEE Xplore](http://ieeexplore.ieee.org/document/7050696/){:target="_blank"} in proceedings of the International Conference on Advances in Pattern Recognition, [ICAPR 2015](http://www.isical.ac.in/~icapr15/AcceptedPapers.php){:target="_blank"}. A [journal paper](docs/publications/IJCV_2017.pdf){:target="_blank"} based on this work is under review at the International Journal of Computer Vision [(IJCV)](https://link.springer.com/journal/11263){:target="_blank"}.

| [**Research paper**]({{site.url}}{{site.baseurl}}/docs/publications/2015_ICAPR.pdf){:target="_blank"}: S. Jonna, V. S. Voleti, R. R. Sahay, and M. S. Kankanhalli, "A Multimodal Approach for Image De-fencing and Depth Inpainting", in _Proc. Int. Conf. Advances in Pattern Recognition_, 2015, pp. 1---6 |

| [**Journal paper**]({{site.url}}{{site.baseurl}}/docs/publications/2017_IJCV.pdf){:target="_blank"}: S. Jonna, S. Satapathy, V. S. Voleti, R. R. Sahay, "Unveiling the scene: A Multimodal Framework for Simultaneous Image Disocclusion and Depth Map Completion using Computational Cameras", _International Journal of Computer Vision_, 2017 | (rejected)

[THESIS](https://github.com/voletiv/MTP_inPainting/blob/master/Vikram_Voleti_Masters_Thesis_compressed.pdf){:target="_blank"} | [Presentation](https://github.com/voletiv/MTP_inPainting/blob/master/Sem_10_MTP_Presentation.pdf){:target="_blank"} | [GitHub](https://github.com/voletiv/MTP_inPainting){:target="_blank"} repository containing thesis, presentation, code files, and results

</div>


<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/kgp.jpg">](http://www.iitkgp.ac.in/){:target="_blank"}

_2012 --- 2013_

### Identification of Bilabial Consonants in Audio and Lip Closures in Video --- B.Tech. Thesis

[**IIT Kharagpur, India**](http://www.iitkgp.ac.in/){:target="_blank"} --- under [Prof. Rajiv Ranjan Sahay](http://www1.iitkgp.ac.in/fac-profiles/showprofile.php?empcode=STmUU&depts_name=EE){:target="_blank"}, Electrical Engineering

I worked on the identification of bilabial consonants in video and audio. The goal was to measure the time offset between the two modes using corresponding time points where bilabials occur. I learnt C++ and the OpenCV library, and detected lip closures in video using the standard Viola-Jones face detector, and a novel algorithm for lip closure detection. I trained a Gaussian Mixture Model in MATLAB on the MFCC features of bilabials in the speech signals of different speakers. A correlation was drawn between the time points of bilabials in audio and video.

[THESIS](https://github.com/voletiv/BTP_GMM_lipClosure/blob/master/Bachelors_Thesis.pdf){:target="_blank"} | [Presentation](https://github.com/voletiv/BTP_GMM_lipClosure/blob/master/Vikram_Voleti_\%5B09EE3501\%5D_BTP_Presentation.pdf){:target="_blank"} | [GitHub](https://github.com/voletiv/BTP_GMM_lipClosure){:target="_blank"} repository containing thesis, presentation, code files, and results

</div>

<br />

# PAST INTERNSHIPS

<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/leuven.png">](https://www.kuleuven.be/english/){:target="_blank"}

_Summer 2013_

### [KU Leuven](https://www.kuleuven.be/english/){:target="_blank"}, Belgium

Supervisor: [Prof. Ingrid Verbauwhede](https://www.kuleuven.be/wieiswie/en/person/00018159){:target="_blank"}, Computer Security & Industrial Applications, ESAT

**Implementation of Carry-Free Arithmetic Operations in FPGA**

I worked on the carry-free implementations of arithmetic operations of addition, subtraction and multiplication. Binary numbers are first converted to a recoded digit format that eliminates carry propagation. I designed the truth tables for this conversion, as well as subsequent addition, subtraction and multiplication. I then simplified the circuits into Product-of-Sums form, and coded them in Verilog. The time taken by these circuits were compared with standard implementation.

A [single-author research paper]({{site.url}}{{site.baseurl}}/docs/publications/2018a_NCC.pdf){:target="_blank"} based on this work has been written.

| [**Research paper**]({{site.url}}{{site.baseurl}}/docs/publications/2018a_NCC.pdf){:target="_blank"}: V. Voleti, "Carry-Free Implementations of Arithmetic Operations in FPGA" |

[Report](https://github.com/voletiv/summer_2013_KULeuven/blob/master/Leuven_Report/KULeuven_Report.pdf){:target="_blank"} | [Presentation](https://github.com/voletiv/summer_2013_KULeuven/blob/master/Leuven_Presentation/Implementation_of_Carry-Free_Arithmetic_Primitives_for_Prime_Field_Elliptic_Curve_Cryptography.pdf){:target="_blank"} | [GitHub](https://github.com/voletiv/summer_2013_KULeuven){:target="_blank"} repository containing report and presentation

</div>


<div class="experience-box" markdown="1">
[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/kgp.jpg">](http://www.iitkgp.ac.in/){:target="_blank"}

_Summer 2012_

### [IIT Kharagpur](http://www.iitkgp.ac.in/){:target="_blank"}, India

Supervisor: [Prof. Aurobinda Routray](http://www.aroutray.org/){:target="_blank"}, Electrical Engineering

**Fingertip Gesture Recognizer using HMMs**

I first implemented Hidden Markov Models (HMM) in MATLAB from scratch, and verified the implementation outputs with those of standard implementation. I then made a simple gesture recognizer in MATLAB using HMMs.

[Report](https://github.com/voletiv/summer_2012_HMM_FingerTipGestureRecognition/blob/master/Vikram\%20Voleti\%20\%5B09EE3501\%5D\%20Summer\%202012\%20Internship\%20Report.pdf){:target="_blank"} | [Presentation](https://github.com/voletiv/summer_2012_HMM_FingerTipGestureRecognition/blob/master/Ppt.pdf){:target="_blank"} | [GitHub](https://github.com/voletiv/summer_2012_HMM_FingerTipGestureRecognition){:target="_blank"} repository containing report, presentation, code files, and results

</div>


<div class="experience-box" markdown="1">

[<img class="experience-picture" src="{{site.url}}{{site.baseurl}}/images/experience/imperial.jpg">](https://www.imperial.ac.uk/){:target="_blank"}

_Summer 2011_

### [Imperial College](https://www.imperial.ac.uk/){:target="_blank"}, London, UK

Supervisor: [Prof. Peter Cheung](http://www.imperial.ac.uk/people/p.cheung){:target="_blank"}, Electrical and Electronic Engineering

**Measurement of Intra-die Power Variation in Sub-nm FPGA’s**

I experimented with an FPGA, and measured the power consumption among the LookUp Tables (LUTs) within it. An automated workflow for the measurement of power across the FPGA was made, by first implementing a circuit in each LUT, measuring the power on an oscilloscope using the JTAG terminals on the FPGA, recording the oscilloscope's readings in MATLAB, and plotting graphs from MATLAB.

[Presentation](https://github.com/voletiv/summer_2011_FPGA_Imperial_College_London/blob/master/An Automated Flow for Intra-Die Power Variation Measurement.pdf){:target="_blank"} | [GitHub](https://github.com/voletiv/summer_2011_FPGA_Imperial_College_London){:target="_blank"} repository containing presentation, certificate, and recommendation letter from Prof. Peter Cheung

</div>

